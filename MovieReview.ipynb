{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_imdb_reviews(movie_url, no_of_pages):\n",
    "    # Set up ChromeOptions for headless browsing\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Necessary for headless mode on some systems\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")  # Set a reasonable window size\n",
    "    chrome_options.add_argument(\"--remote-debugging-port=0\")\n",
    "    \n",
    "    # Create a headless WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        # Navigate to the movie URL using the headless WebDriver\n",
    "        driver.get(movie_url)\n",
    "\n",
    "        # Extracting movie name\n",
    "        movie_element = driver.find_element(By.CSS_SELECTOR, \"a[itemprop='url']\")\n",
    "        movie_name = movie_element.text.strip()\n",
    "\n",
    "        # Print the movie name\n",
    "        print(f\"Movie Name: {movie_name}\")\n",
    "\n",
    "        NoOfLoadMore = 0\n",
    "        if int(no_of_pages > 0):\n",
    "            per_cycle_increment = 100 / int(no_of_pages)\n",
    "\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                if int(no_of_pages) > 0:\n",
    "                    if int(per_cycle_increment) * NoOfLoadMore < 100:\n",
    "                        print(\"Progress: \", int(per_cycle_increment) * NoOfLoadMore)\n",
    "                    if NoOfLoadMore > int(no_of_pages):\n",
    "                        break\n",
    "                # Extracting \"load more\" button and clicking it\n",
    "                loadMoreElement = driver.find_element(By.ID, \"load-more-trigger\")\n",
    "                loadMoreElement.click()\n",
    "                NoOfLoadMore += 1\n",
    "\n",
    "                # Wait for dynamic content to load (adjust as needed)\n",
    "                driver.implicitly_wait(5)\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                # Break the loop when the \"load more\" button is not found\n",
    "                print(\"No such element found. Exiting the loop.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                # Print additional information about the error\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "        # Extracting review text after all \"load more\" clicks\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        reviews = []\n",
    "\n",
    "        review_divs = soup.find_all(\"a\", class_=\"title\")\n",
    "        date_divs = soup.find_all(\"span\", class_=\"review-date\")\n",
    "\n",
    "        for review_div, date_div in zip(review_divs, date_divs):\n",
    "            review_text = review_div.get_text(strip=True)\n",
    "            review_date = date_div.get_text(strip=True)\n",
    "\n",
    "            review = {\"text\": review_text, \"date\": review_date}\n",
    "            reviews.append(review)\n",
    "\n",
    "\n",
    "        return reviews\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the WebDriver after scraping\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDb movie URL (replace with the actual movie URL)\n",
    "IMDB_Code = \"tt12915716\"\n",
    "no_of_pages = 5\n",
    "movie_url = f'https://www.imdb.com/title/{IMDB_Code}/reviews'\n",
    "\n",
    "# Scrape IMDb movie reviews\n",
    "movie_reviews = scrape_imdb_reviews(movie_url, no_of_pages)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(movie_reviews)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text cleaning\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())  # Convert to lowercase and remove special characters\n",
    "    return cleaned_text\n",
    "\n",
    "# Function for tokenization\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# Function for removing stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Function for lemmatization\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Preprocessing\n",
    "df[\"text_cleaned\"] = df[\"text\"].apply(clean_text)\n",
    "df[\"tokens\"] = df[\"text_cleaned\"].apply(tokenize_text)\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(remove_stopwords)\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis using VADER\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "df['compound'] = df['text_cleaned'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
    "\n",
    "# Classify sentiments based on compound score\n",
    "df['predicted_sentiment'] = df['compound'].apply(lambda x: 'positive' if x >= 0 else 'negative')\n",
    "\n",
    "df.head()  # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "accuracy = accuracy_score(df['predicted_sentiment'], df['predicted_sentiment'])\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(df['predicted_sentiment'], df['predicted_sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_sentiment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the review date as a datetime object\n",
    "df[\"review_date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df_filtered = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"results\"\n",
    "\n",
    "# Create a line graph showing the number of positive and negative reviews over time\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "df_filtered.groupby(\n",
    "    [df_filtered[\"review_date\"].dt.date, \"predicted_sentiment\"]\n",
    ").size().unstack().plot(kind=\"line\", ax=ax1)\n",
    "ax1.set_title(f\"Number of Positive and Negative Reviews Over Time ({IMDB_Code})\")\n",
    "ax1.set_xlabel(\"Review Date\")\n",
    "ax1.set_ylabel(\"Number of Reviews\")\n",
    "\n",
    "# Visualize the distribution of predicted sentiments\n",
    "colors = [\n",
    "    \"green\" if col.lower() == \"positive\" else \"red\"\n",
    "    for col in df_filtered[\"predicted_sentiment\"].value_counts().index\n",
    "]\n",
    "\n",
    "df_filtered[\"predicted_sentiment\"].value_counts().plot(kind=\"bar\", color=colors, ax=ax2)\n",
    "ax2.set_title(f\"Distribution of Predicted Sentiments in IMDb Reviews ({IMDB_Code})\")\n",
    "ax2.set_xlabel(\"Predicted Sentiment\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to files\n",
    "plot_file_path = os.path.join(save_path, f\"sentiment_analysis_for_{IMDB_Code}.png\")\n",
    "plt.savefig(plot_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(save_path, f\"movie_reviews_with_sentiment_for_{IMDB_Code}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a message indicating positive sentiment\n",
    "more_counts = df['predicted_sentiment'].value_counts().index \n",
    "\n",
    "if \"positive\" == more_counts[0]:\n",
    "    print(\"The movie has positive reviews! You should consider watching it.\")\n",
    "else:\n",
    "    print(\"The sentiment analysis did not identify a clear positive sentiment in the reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Write-up\n",
    "\n",
    "### Movie Review Sentiment Analysis with Dynamic Loading\n",
    "\n",
    "This project focuses on analyzing the sentiment of movie reviews from IMDb, incorporating dynamic loading of reviews to ensure a comprehensive analysis. The objective is to provide a nuanced understanding of audience sentiments regarding a specific movie over time.\n",
    "\n",
    "#### Data Acquisition:\n",
    "\n",
    "The data collection process involves web scraping IMDb movie reviews using Python's Selenium WebDriver. The headless browser navigates to the movie's review page, dynamically loads reviews by clicking the \"load more\" button, and extracts both review text and dates. The scraping process is iterative, allowing users to specify the number of additional pages to load.\n",
    "\n",
    "#### Data Processing and Cleaning:\n",
    "\n",
    "Following data acquisition, the collected text data undergoes standard text cleaning procedures. Techniques such as text normalization, tokenization, stopword removal, and lemmatization are applied to ensure a clean and consistent dataset for subsequent analysis.\n",
    "\n",
    "#### Sentiment Analysis Implementation:\n",
    "\n",
    "Sentiment analysis is conducted using the VADER sentiment analysis tool. The compound score provided by VADER is utilized to classify reviews as either 'positive' or 'negative.' The dynamic loading of reviews allows for an analysis of sentiments over time, providing insights into evolving audience opinions.\n",
    "\n",
    "#### Results Visualization:\n",
    "\n",
    "The project employs Matplotlib to visualize the outcomes effectively. Two plots are generated: the first showcases the number of positive and negative reviews over time, while the second illustrates the distribution of predicted sentiments. These visualizations offer a clear representation of sentiment trends and the overall sentiment distribution.\n",
    "\n",
    "In conclusion, this project not only performs sentiment analysis on movie reviews but also incorporates dynamic loading to capture temporal trends in audience sentiments. The detailed methodology ensures transparency, reproducibility, and accuracy in sentiment classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
